@article{DBLP:journals/corr/MnihKSGAWR13,
  author     = {Volodymyr Mnih and
                Koray Kavukcuoglu and
                David Silver and
                Alex Graves and
                Ioannis Antonoglou and
                Daan Wierstra and
                Martin A. Riedmiller},
  title      = {Playing Atari with Deep Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/1312.5602},
  year       = {2013},
  url        = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint     = {1312.5602},
  timestamp  = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{soft_update,
  title         = {Continuous control with deep reinforcement learning},
  author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  year          = {2019},
  eprint        = {1509.02971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@book{sutton,
  added-at  = {2019-07-13T10:11:53.000+0200},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition   = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords  = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title     = {Reinforcement Learning: An Introduction},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  year      = {2018 }
}

@misc{hubert,
  url    = {https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3},
  year   = {2019},
  author = {George Seif},
  title  = {Understanding the 3 most common loss functions for Machine Learning Regression}
}

@misc{discretization_tech,
  url    = {https://towardsdatascience.com/an-introduction-to-discretization-in-data-science-55ef8c9775a2},
  author = {Rohan Gupta},
  title  = {An Introduction to Discretization Techniques for Data Scientists},
  year   = {2019}
}

@article{states_vs_rewards,
  author  = {Jan Glascher and
             Peter Dayan and
             John P. O'Doherty},
  journal = {},
  number  = {4},
  title   = {States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning},
  volume  = {},
  year    = {2021},
  url     = {http://dx.doi.org/10.1016/j.neuron.2010.04.016}
}

@article{feedback_control,
  author  = {Hafner R and
             Riedmiller M.},
  journal = {},
  number  = {},
  title   = {Reinforcement learning in feedback control},
  volume  = {},
  year    = {2011},
  url     = {https://doi.org/10.1007/s10994-011-5235-x}
}

@article{human_control,
  author  = {Mnih V. and
             Kavukcuoglu K. and
             Silver D.},
  journal = {Nature},
  number  = {529-533},
  title   = {Human-level control through deep reinforcement learning},
  volume  = {518},
  year    = {2015},
  url     = {https://doi.org/10.1038/nature14236}
}


@inproceedings{ddpg,
  title     = {Deterministic Policy Gradient Algorithms},
  author    = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  pages     = {387--395},
  year      = {2014},
  editor    = {Xing, Eric P. and Jebara, Tony},
  volume    = {32},
  number    = {1},
  series    = {Proceedings of Machine Learning Research},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v32/silver14.pdf},
  url       = {https://proceedings.mlr.press/v32/silver14.html},
  abstract  = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.}
}


@article{qlearning,
  author  = {Watkings and Dayan P. and C.J.C.H.},
  journal = {Mach Learn},
  number  = {279-292},
  title   = {Q-learning},
  volume  = {8},
  year    = {1992}
}

@article{noise,
  author  = {Wawrzynski, Pawel},
  year    = {2015},
  month   = {04},
  pages   = {91-95},
  title   = {Control Policy with Autocorrelated Noise in Reinforcement Learning for Robotics},
  volume  = {5},
  journal = {International Journal of Machine Learning and Computing},
  doi     = {10.7763/IJMLC.2015.V5.489}
}

@article{replay,
  author  = {Pawel Wawrzynski and Ajay Kumar Tanwani},
  journal = {PubMed},
  number  = {156-67},
  title   = {Autonomous reinforcement learning with experience replay},
  volume  = {41},
  year    = {2012}
}

@article{function_approximation,
  author  = {Leemon Baird},
  journal = {Proceedings of the 12th International Conference on Machine Learning (ICML 1995)},
  number  = {},
  title   = {Reinforcement learning with function approximation},
  volume  = {},
  year    = {1995},
  pages   = {30-37}
}

@inproceedings{neural_network,
  title  = {Reinforcement learning for robots using neural networks},
  author = {Longxin Lin},
  year   = {1992}
}

@inproceedings{convergence,
  author    = {Maei, Hamid and Szepesv\'{a}ri, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},
  volume    = {22},
  year      = {2009}
}


@inproceedings{learning_control,
  author          = {Maei, Hamid Reza and Szepesv{\'a}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S.},
  title           = {Toward Off-Policy Learning Control with Function Approximation},
  booktitle       = {Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML 2010)},
  year            = {2010},
  editor          = {F{\"u}rnkranz, Johannes and Joachims, Thorsten},
  publisher       = {Omnipress},
  pages           = {719--726},
  url             = {http://www.ualberta.ca/~maei/Pub_PDFs/ICML10_controlGQ.pdf},
  bib2html_rescat = {Parameter}
}

@article{temporal_diff,
  author  = {Tsitsiklis, J.N. and Van Roy, B.},
  journal = {IEEE Transactions on Automatic Control},
  title   = {An analysis of temporal-difference learning with function approximation},
  year    = {1997},
  volume  = {42},
  number  = {5},
  pages   = {674-690},
  doi     = {10.1109/9.580874}
}
